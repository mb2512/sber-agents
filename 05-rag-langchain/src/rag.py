import logging
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI
from config import config

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
vector_store = None
retriever = None

# –ö–µ—à–∏ –¥–ª—è –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ LLM –∫–ª–∏–µ–Ω—Ç–æ–≤
_conversational_answering_prompt = None
_retrieval_query_transform_prompt = None
_llm_query_transform = None
_llm = None

def initialize_retriever():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è retriever –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    global retriever
    if vector_store is None:
        logger.error("Cannot initialize retriever: vector_store is None")
        return False
    
    retriever = vector_store.as_retriever(search_kwargs={'k': config.RETRIEVER_K})
    logger.info(f"Retriever initialized with k={config.RETRIEVER_K}")
    return True

def format_chunks(chunks):
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏
    """
    if not chunks:
        return "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
    
    formatted_parts = []
    for i, chunk in enumerate(chunks, 1):
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        source = chunk.metadata.get('source', 'Unknown')
        page = chunk.metadata.get('page', 'N/A')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏
        source_name = source.split('/')[-1] if '/' in source else source
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —á–∞–Ω–∫
        formatted_parts.append(
            f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i}: {source_name}, —Å—Ç—Ä. {page}]\n{chunk.page_content}"
        )
    
    return "\n\n---\n\n".join(formatted_parts)

def _load_prompts():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    global _conversational_answering_prompt, _retrieval_query_transform_prompt
    
    if _conversational_answering_prompt is not None:
        return _conversational_answering_prompt, _retrieval_query_transform_prompt
    
    try:
        conversation_system_text = config.load_prompt(config.CONVERSATION_SYSTEM_PROMPT_FILE)
        query_transform_text = config.load_prompt(config.QUERY_TRANSFORM_PROMPT_FILE)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π context
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ChatPromptTemplate –Ω–∞–ø—Ä—è–º—É—é —Å–æ —Å–ø–∏—Å–∫–æ–º –∫–æ—Ä—Ç–µ–∂–µ–π (–∫–∞–∫ –≤ –ø—Ä–∏–º–µ—Ä–µ –∏–∑ notebook)
        # –≠—Ç–æ –¥–æ–ª–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é {context} –∏–∑ RunnablePassthrough.assign
        _conversational_answering_prompt = ChatPromptTemplate(
            [
                ("system", conversation_system_text),  # –°—Ç—Ä–æ–∫–∞ —Å {context} –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞
                ("placeholder", "{messages}")  # –ò—Å–ø–æ–ª—å–∑—É–µ–º placeholder –≤–º–µ—Å—Ç–æ MessagesPlaceholder
            ]
        )
        
        _retrieval_query_transform_prompt = ChatPromptTemplate.from_messages(
            [
                MessagesPlaceholder(variable_name="messages"),
                ("user", query_transform_text),
            ]
        )
        
        logger.info("Prompts loaded successfully")
        return _conversational_answering_prompt, _retrieval_query_transform_prompt
        
    except FileNotFoundError as e:
        logger.error(f"Prompt file not found: {e}")
        raise
    except Exception as e:
        logger.error(f"Error loading prompts: {e}", exc_info=True)
        raise

def _get_llm_query_transform():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –¥–ª—è query transformation —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    global _llm_query_transform
    if _llm_query_transform is None:
        _llm_query_transform = ChatOpenAI(
            model=config.MODEL_QUERY_TRANSFORM,
            temperature=0.4,
            openai_api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL,
            timeout=config.REQUEST_TIMEOUT,
            max_retries=2
        )
        logger.info(f"Query transform LLM initialized: {config.MODEL_QUERY_TRANSFORM}")
    return _llm_query_transform

def _get_llm():
    """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π LLM —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    global _llm
    if _llm is None:
        _llm = ChatOpenAI(
            model=config.MODEL,
            temperature=0.9,
            openai_api_key=config.OPENAI_API_KEY,
            base_url=config.OPENAI_BASE_URL,
            timeout=config.REQUEST_TIMEOUT,
            max_retries=2
        )
        logger.info(f"Main LLM initialized: {config.MODEL}")
    return _llm

def get_retrieval_query_transformation_chain():
    """–¶–µ–ø–æ—á–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞"""
    _, retrieval_query_transform_prompt = _load_prompts()
    return (
        retrieval_query_transform_prompt
        | _get_llm_query_transform()
        | StrOutputParser()
    )

def get_rag_chain():
    """–§–∏–Ω–∞–ª—å–Ω–∞—è RAG-—Ü–µ–ø–æ—á–∫–∞ —Å query transformation"""
    if retriever is None:
        raise ValueError("Retriever not initialized")
    
    conversational_answering_prompt, _ = _load_prompts()
    
    def format_docs(docs):
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        return format_chunks(docs)
    
    def log_prompt_input(input_dict):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)"""
        context = input_dict.get("context", "NO CONTEXT!")
        messages_count = len(input_dict.get("messages", []))
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º INFO –¥–ª—è –≤–∞–∂–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç–ª–∞–¥–∫–∏
        logger.info(f"üîç Prompt input - context length: {len(context) if context != 'NO CONTEXT!' else 0}, messages: {messages_count}")
        if context == "NO CONTEXT!":
            logger.error("‚ùå CONTEXT IS MISSING! This is the problem!")
        else:
            logger.info(f"‚úÖ Context preview: {context[:500]}...")
        return input_dict
    
    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É: —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ -> –ø–æ–∏—Å–∫ -> —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ -> –æ—Ç–≤–µ—Ç
    retrieval_chain = (
        get_retrieval_query_transformation_chain() 
        | retriever 
        | format_docs
    )
    
    def log_formatted_messages(formatted_messages):
        """–õ–æ–≥–∏—Ä—É–µ–º –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –≤ LLM"""
        if hasattr(formatted_messages, 'messages'):
            for i, msg in enumerate(formatted_messages.messages):
                if hasattr(msg, 'content'):
                    content_preview = str(msg.content)[:500] if msg.content else "Empty"
                    logger.info(f"üì§ Message {i} to LLM ({type(msg).__name__}): {content_preview}...")
                    if "context" in content_preview.lower() and "–∫–æ–Ω—Ç–µ–∫—Å—Ç" not in content_preview.lower():
                        logger.warning("‚ö†Ô∏è Context variable might not be substituted!")
        return formatted_messages
    
    return (
        RunnablePassthrough.assign(context=retrieval_chain)
        | log_prompt_input  # –õ–æ–≥–∏—Ä—É–µ–º, —á—Ç–æ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –≤ –ø—Ä–æ–º–ø—Ç
        | conversational_answering_prompt
        | log_formatted_messages  # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        | _get_llm()
        | StrOutputParser()
    )

async def rag_answer(messages):
    """
    –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç RAG —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞
    
    Args:
        messages: —Å–ø–∏—Å–æ–∫ LangChain messages (HumanMessage, AIMessage)
    
    Returns:
        str: –æ—Ç–≤–µ—Ç –æ—Ç RAG
    """
    if vector_store is None or retriever is None:
        logger.error("Vector store or retriever not initialized")
        raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
    
    # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤–æ–ø—Ä–æ—Å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    last_message = messages[-1].content if messages else "No messages"
    logger.info(f"Processing RAG query: {last_message[:100]}...")
    
    try:
        # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ retriever —Ä–∞–±–æ—Ç–∞–µ—Ç
        transform_chain = get_retrieval_query_transformation_chain()
        transformed_query = await transform_chain.ainvoke({"messages": messages})
        logger.info(f"Transformed query: {transformed_query[:200]}...")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω–æ–µ –∏–ª–∏ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ question –≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        # –≠—Ç–æ –±—ã—Å—Ç—Ä–µ–µ –∏ —Ç–æ—á–Ω–µ–µ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ JSON
        exact_match_chunk = None
        query_lower = last_message.lower().strip()
        query_words = set(query_lower.split())
        
        if vector_store:
            try:
                # –ò—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º—É –∑–∞–ø—Ä–æ—Å—É (–Ω–µ transformed_query!)
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª—å—à–æ–π k, —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                logger.info(f"üîç Searching for exact match by question metadata (query: '{query_lower}')...")
                
                # –°–ø–æ—Å–æ–± 1: –∏—â–µ–º —á–µ—Ä–µ–∑ similarity_search —Å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∑–∞–ø—Ä–æ—Å–æ–º –∏ –±–æ–ª—å—à–∏–º k
                search_chunks = await vector_store.asimilarity_search(query_lower, k=200)
                logger.info(f"üîç Got {len(search_chunks)} chunks from similarity search with original query")
                
                for chunk in search_chunks:
                    question = chunk.metadata.get('question', '').lower().strip()
                    if question:
                        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Å —É—á–µ—Ç–æ–º –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
                        question_normalized = question.rstrip('?').rstrip('!').rstrip('.')
                        query_normalized = query_lower.rstrip('?').rstrip('!').rstrip('.')
                        if question_normalized == query_normalized or question == query_lower:
                            exact_match_chunk = chunk
                            logger.info(f"‚úÖ Found exact match by question: '{question}'")
                            break
                
                # –°–ø–æ—Å–æ–± 2: –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ, –∏—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if not exact_match_chunk:
                    logger.info(f"üîç Searching for partial match...")
                    for chunk in search_chunks:
                        question = chunk.metadata.get('question', '').lower().strip()
                        if question:
                            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ question –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∑–∞–ø—Ä–æ—Å–∞
                            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞
                            stop_words = {'–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—Ç–æ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–¥–ª—è', '–Ω—É–∂–Ω—ã', '–Ω—É–∂–µ–Ω', '–Ω—É–∂–Ω–∞', '–Ω—É–∂–Ω–æ', '—á—Ç–æ–±—ã', '–º–æ–∂–Ω–æ', '–º–æ–∂–Ω–æ', '–ª–∏'}
                            query_words_filtered = {w for w in query_words if w not in stop_words and len(w) > 2}
                            question_words = set(question.split())
                            question_words_filtered = {w for w in question_words if w not in stop_words and len(w) > 2}
                            
                            # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 70% –∑–Ω–∞—á–∏–º—ã—Ö —Å–ª–æ–≤ –∑–∞–ø—Ä–æ—Å–∞ –µ—Å—Ç—å –≤ question
                            if len(query_words_filtered) > 0:
                                common_words = query_words_filtered & question_words_filtered
                                match_ratio = len(common_words) / len(query_words_filtered)
                                if match_ratio >= 0.7:
                                    exact_match_chunk = chunk
                                    logger.info(f"‚úÖ Found partial match by question: '{question}' (match ratio: {match_ratio:.2f}, common words: {common_words})")
                                    break
                
                # –°–ø–æ—Å–æ–± 3: –µ—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º —Å transformed_query
                if not exact_match_chunk:
                    logger.info(f"üîç Trying with transformed query...")
                    all_chunks = await vector_store.asimilarity_search(transformed_query, k=100)
                    logger.info(f"üîç Got {len(all_chunks)} chunks from similarity search with transformed query")
                    for chunk in all_chunks:
                        question = chunk.metadata.get('question', '').lower().strip()
                        if question:
                            question_normalized = question.rstrip('?').rstrip('!').rstrip('.')
                            query_normalized = query_lower.rstrip('?').rstrip('!').rstrip('.')
                            if question_normalized == query_normalized or question == query_lower:
                                exact_match_chunk = chunk
                                logger.info(f"‚úÖ Found exact match via transformed query: '{question}'")
                                break
            except Exception as e:
                logger.warning(f"Could not search for exact match: {e}", exc_info=True)
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ —á–µ—Ä–µ–∑ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        retrieved_chunks = await retriever.ainvoke(transformed_query)
        
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, —Å—Ç–∞–≤–∏–º –µ–≥–æ –ø–µ—Ä–≤—ã–º
        if exact_match_chunk:
            # –£–¥–∞–ª—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–∑ —Å–ø–∏—Å–∫–∞, –µ—Å–ª–∏ –æ–Ω–æ —Ç–∞–º –µ—Å—Ç—å
            retrieved_chunks = [c for c in retrieved_chunks if c != exact_match_chunk]
            # –°—Ç–∞–≤–∏–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–µ—Ä–≤—ã–º
            retrieved_chunks = [exact_match_chunk] + retrieved_chunks[:config.RETRIEVER_K - 1]
            logger.info(f"‚úÖ Using exact match as first chunk, total chunks: {len(retrieved_chunks)}")
        else:
            # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —á–∞–Ω–∫–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ question
            # –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —É–±—Ä–∞—Ç—å –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏ –∏–∑ PDF
            filtered_chunks = []
            for chunk in retrieved_chunks:
                if chunk.metadata.get('question'):
                    filtered_chunks.append(chunk)
            
            if filtered_chunks:
                logger.info(f"‚úÖ Filtered to {len(filtered_chunks)} chunks with question metadata (from {len(retrieved_chunks)} total)")
                retrieved_chunks = filtered_chunks[:config.RETRIEVER_K]
            else:
                logger.warning(f"‚ö†Ô∏è No chunks with question metadata found, using all {len(retrieved_chunks)} chunks")
        
        logger.info(f"Retrieved {len(retrieved_chunks)} chunks")
        if retrieved_chunks:
            # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            for i, chunk in enumerate(retrieved_chunks):
                question = chunk.metadata.get('question', 'N/A')
                category = chunk.metadata.get('category', 'N/A')
                preview = chunk.page_content[:150].replace('\n', ' ')
                logger.info(f"Chunk {i+1}: Q='{question}', Category='{category}', Preview='{preview}...'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ —á–∞–Ω–∫–∞—Ö –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞—Ä—Ç–∞—Ö
            has_card_info = any("–∫–∞—Ä—Ç" in chunk.page_content.lower() or "card" in chunk.page_content.lower() 
                              for chunk in retrieved_chunks)
            logger.info(f"Chunks contain card-related info: {has_card_info}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞
            query_lower = last_message.lower()
            exact_match = any(query_lower in chunk.page_content.lower() or 
                            chunk.metadata.get('question', '').lower() in query_lower
                            for chunk in retrieved_chunks)
            logger.info(f"Exact question match found: {exact_match}")
        else:
            logger.warning("‚ö†Ô∏è No chunks retrieved! This might be the problem.")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        formatted_context = format_chunks(retrieved_chunks)
        logger.info(f"üìù Formatted context length: {len(formatted_context)} chars")
        logger.info(f"üìù Formatted context preview (first 800 chars): {formatted_context[:800]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        context_lower = formatted_context.lower()
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        import string
        stop_words = {'–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—Ç–æ', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º', '–¥–ª—è', '–Ω—É–∂–Ω—ã', '–Ω—É–∂–µ–Ω', '–Ω—É–∂–Ω–∞', '–Ω—É–∂–Ω–æ', '—á—Ç–æ–±—ã', '–º–æ–∂–Ω–æ', '–ª–∏'}
        query_keywords = [w.rstrip('?').rstrip('!').rstrip('.').rstrip(',') 
                         for w in query_lower.split() 
                         if len(w.rstrip('?').rstrip('!').rstrip('.').rstrip(',')) > 2 
                         and w.rstrip('?').rstrip('!').rstrip('.').rstrip(',') not in stop_words]
        found_keywords = [kw for kw in query_keywords if kw in context_lower]
        logger.info(f"üîç Query keywords: {query_keywords}, Found in context: {found_keywords} ({len(found_keywords)}/{len(query_keywords)})")
        
        # –ï—Å–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, —ç—Ç–æ –ø–ª–æ—Ö–æ–π –∑–Ω–∞–∫ - –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
        if len(found_keywords) == 0 and len(query_keywords) > 0:
            logger.warning(f"‚ö†Ô∏è None of the query keywords found in context! This suggests wrong chunks were retrieved.")
            logger.warning(f"‚ö†Ô∏è Query was: '{last_message}', but context contains: '{formatted_context[:500]}...'")
        
        # –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é —Ü–µ–ø–æ—á–∫—É, –∫–æ—Ç–æ—Ä–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
        # –≤–º–µ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ retriever
        conversational_answering_prompt, _ = _load_prompts()
        
        # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é, –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        def get_context(input_dict):
            return formatted_context
        
        # –°–æ–∑–¥–∞–µ–º —Ü–µ–ø–æ—á–∫—É —Å –Ω–∞—à–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        custom_rag_chain = (
            RunnablePassthrough.assign(context=get_context)
            | conversational_answering_prompt
            | _get_llm()
            | StrOutputParser()
        )
        
        # –í—ã–∑—ã–≤–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é —Ü–µ–ø–æ—á–∫—É —Å —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏
        input_data = {"messages": messages}
        logger.debug(f"Input to RAG chain: messages count={len(messages)}")
        
        result = await custom_rag_chain.ainvoke(input_data)
        
        logger.info(f"RAG response generated, length: {len(result)} chars")
        logger.debug(f"RAG response: {result[:200]}...")
        if not result or len(result.strip()) == 0:
            logger.warning("‚ö†Ô∏è Empty response from RAG chain!")
        if "–Ω–µ –Ω–∞—à–µ–ª" in result.lower() or "–Ω–µ –Ω–∞—à—ë–ª" in result.lower():
            logger.warning("‚ö†Ô∏è LLM returned 'not found' response - context might not be passed correctly!")
        return result
        
    except Exception as e:
        logger.error(f"Error in rag_answer: {e}", exc_info=True)
        raise

def get_vector_store_stats():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    if vector_store is None:
        return {"status": "not initialized", "count": 0}
    
    doc_count = len(vector_store.store) if hasattr(vector_store, 'store') else 0
    return {"status": "initialized", "count": doc_count}

