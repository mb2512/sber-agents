import logging
from pathlib import Path
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import JSONLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import InMemoryVectorStore
from config import config

logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç OllamaEmbeddings (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
try:
    from langchain_ollama import OllamaEmbeddings
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    logger.warning("langchain-ollama not installed. Ollama embeddings will not be available.")

def load_pdf_documents(data_dir: str) -> list:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    pages = []
    data_path = Path(data_dir)
    
    if not data_path.exists():
        logger.warning(f"Directory {data_dir} does not exist")
        return pages
    
    pdf_files = list(data_path.glob("*.pdf"))
    logger.info(f"Found {len(pdf_files)} PDF files in {data_dir}")
    
    for pdf_file in pdf_files:
        loader = PyPDFLoader(str(pdf_file))
        pages.extend(loader.load())
        logger.info(f"Loaded {pdf_file.name}")
    
    return pages

def load_json_documents(json_file_path: str) -> list:
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞ —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏-–æ—Ç–≤–µ—Ç–∞–º–∏
    –ö–∞–∂–¥–∞—è –ø–∞—Ä–∞ Q&A —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–º —á–∞–Ω–∫–æ–º
    """
    from pathlib import Path
    import json
    
    json_path = Path(json_file_path)
    if not json_path.exists():
        logger.warning(f"JSON file {json_file_path} does not exist")
        return []
    
    try:
        # JSONLoader —Å jq_schema –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è full_text –∏–∑ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –º–∞—Å—Å–∏–≤–∞
        loader = JSONLoader(
            file_path=str(json_path),
            jq_schema='.[].full_text',  # –ò–∑–≤–ª–µ–∫–∞–µ–º full_text –∏–∑ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
            text_content=False
        )
        
        documents = loader.load()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É
        for i, doc in enumerate(documents):
            if i < len(data):
                item = data[i]
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                doc.metadata.update({
                    'question': item.get('question', ''),
                    'category': item.get('category', ''),
                    'url': item.get('url', '')
                })
                # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                if i < 3:
                    logger.info(f"Sample document {i+1}: question='{item.get('question', '')}', category='{item.get('category', '')}'")
        
        logger.info(f"Loaded {len(documents)} Q&A pairs from JSON")
        return documents
    except ImportError as e:
        if "jq" in str(e).lower():
            logger.error("jq package is required for JSONLoader. Install it with: uv sync")
            logger.warning("Falling back to manual JSON parsing...")
            # Fallback: —Ä—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ JSON
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                from langchain_core.documents import Document
                documents = []
                for i, item in enumerate(data):
                    if 'full_text' in item:
                        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
                        metadata = {
                            'source': str(json_path),
                            'type': 'json',
                            'question': item.get('question', ''),
                            'category': item.get('category', ''),
                            'url': item.get('url', '')
                        }
                        doc = Document(
                            page_content=item['full_text'],
                            metadata=metadata
                        )
                        documents.append(doc)
                        # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
                        if i < 3:
                            logger.info(f"Sample document {i+1}: question='{metadata['question']}', category='{metadata['category']}'")
                
                logger.info(f"Loaded {len(documents)} Q&A pairs from JSON (manual parsing)")
                return documents
            except Exception as fallback_error:
                logger.error(f"Error in fallback JSON parsing: {fallback_error}")
                return []
        else:
            raise
    except Exception as e:
        logger.error(f"Error loading JSON documents: {e}", exc_info=True)
        return []

def split_documents(pages: list) -> list:
    """–†–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    # –°–µ–ø–∞—Ä–∞—Ç–æ—Ä—ã –¥–ª—è –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏–≤–∞—Ç—å –ø–æ: –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫, –æ–¥–∏–Ω–∞—Ä–Ω—ã–º, –ø—Ä–æ–±–µ–ª–∞–º
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        separators=[
            "\n\n\n",    # –¢—Ä–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å - –æ–±—ã—á–Ω–æ —Ä–∞–∑–¥–µ–ª—ã
            "\n\n",      # –î–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å - –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            "\n",        # –û–¥–∏–Ω–∞—Ä–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å
            ". ",        # –ö–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            " ",         # –ü—Ä–æ–±–µ–ª—ã
            ""           # –°–∏–º–≤–æ–ª—ã
        ],
        keep_separator=True  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    )
    chunks = text_splitter.split_documents(pages)
    logger.info(f"Split into {len(chunks)} chunks")
    return chunks

def create_vector_store(chunks: list):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞"""
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "aroxima/" –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç "ollama", –∏—Å–ø–æ–ª—å–∑—É–µ–º Ollama
        use_ollama = (
            OLLAMA_AVAILABLE and 
            (config.EMBEDDING_MODEL.startswith("aroxima/") or 
             "ollama" in config.EMBEDDING_MODEL.lower() or
             config.EMBEDDING_MODEL.endswith(":latest"))
        )
        
        if use_ollama:
            logger.info(f"Using Ollama embeddings with model: {config.EMBEDDING_MODEL}")
            embeddings = OllamaEmbeddings(
                model=config.EMBEDDING_MODEL
            )
        else:
            logger.info(f"Using OpenAI-compatible embeddings with model: {config.EMBEDDING_MODEL}")
            embeddings = OpenAIEmbeddings(
                model=config.EMBEDDING_MODEL,
                openai_api_key=config.OPENAI_API_KEY,
                base_url=config.OPENAI_BASE_URL,
                timeout=config.REQUEST_TIMEOUT,
                max_retries=2
            )
        
        logger.info(f"Creating vector store with {len(chunks)} chunks using model {config.EMBEDDING_MODEL}")
        vector_store = InMemoryVectorStore.from_documents(
            documents=chunks,
            embedding=embeddings
        )
        logger.info(f"Created vector store with {len(chunks)} chunks")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å - –∏—Å–ø–æ–ª—å–∑—É–µ–º similarity_search, —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±
        try:
            # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –∫–æ—Ç–æ—Ä—ã–π –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ JSON
            test_chunks = vector_store.similarity_search("–ö–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –∫–∞—Ä—Ç—É?", k=10)
            logger.info(f"üîç Testing metadata preservation: found {len(test_chunks)} chunks for test query")
            found_question = False
            for i, chunk in enumerate(test_chunks[:5]):
                if hasattr(chunk, 'metadata'):
                    question = chunk.metadata.get('question', '')
                    if question:
                        logger.info(f"‚úÖ Test chunk {i+1} metadata: question='{question}'")
                        if '–∑–∞–∫–∞–∑–∞—Ç—å' in question.lower() and '–∫–∞—Ä—Ç—É' in question.lower():
                            found_question = True
                    else:
                        logger.warning(f"‚ö†Ô∏è Test chunk {i+1} has metadata but no 'question' field")
                else:
                    logger.warning(f"‚ö†Ô∏è Test chunk {i+1} has no metadata attribute")
            
            if found_question:
                logger.info("‚úÖ Metadata preservation verified - found expected question in test search")
            else:
                logger.warning("‚ö†Ô∏è Could not find expected question '–ö–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –∫–∞—Ä—Ç—É?' in test search - metadata might not be preserved correctly")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not test metadata preservation: {e}")
        
        return vector_store
    except Exception as e:
        logger.error(f"Error creating vector store: {e}", exc_info=True)
        raise

async def reindex_all():
    """–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (PDF + JSON)"""
    logger.info("Starting full reindexing...")
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º PDF –¥–æ–∫—É–º–µ–Ω—Ç—ã
        pdf_pages = load_pdf_documents(config.DATA_DIR)
        if not pdf_pages:
            logger.warning("No PDF documents found to index")
        
        pdf_chunks = split_documents(pdf_pages) if pdf_pages else []
        
        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º JSON —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏-–æ—Ç–≤–µ—Ç–∞–º–∏
        json_file = f"{config.DATA_DIR}/sberbank_help_documents.json"
        json_chunks = load_json_documents(json_file)
        
        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
        all_chunks = pdf_chunks + json_chunks
        
        if not all_chunks:
            logger.warning("No documents found to index")
            return None
        
        logger.info(f"Total chunks to index: {len(all_chunks)} (PDF: {len(pdf_chunks)}, JSON: {len(json_chunks)})")
            
        # 4. –°–æ–∑–¥–∞—ë–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        logger.info("Creating vector store...")
        vector_store = create_vector_store(all_chunks)
        logger.info("Reindexing completed successfully")
        return vector_store
        
    except FileNotFoundError as e:
        logger.error(f"File not found: {e}")
        return None
    except ValueError as e:
        logger.error(f"Configuration error: {e}. Check your .env file and API keys.")
        return None
    except Exception as e:
        logger.error(f"Error during reindexing: {e}", exc_info=True)
        return None

