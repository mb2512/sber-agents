# Отчет о проведенных экспериментах RAG системы

## Обзор

В данном отчете представлены результаты экспериментов по оценке качества различных конфигураций RAG (Retrieval-Augmented Generation) системы для Telegram-бота Сбербанка. Были протестированы три конфигурации: semantic retrieval, hybrid retrieval и hybrid retrieval с reranking.

## Используемые модели

### LLM (Large Language Models)

**Основная модель генерации:**
- **Модель:** `accounts/fireworks/models/gpt-oss-120b`
- **Провайдер:** Fireworks AI
- **Параметры:** 120B параметров
- **Назначение:** Генерация ответов на основе извлеченного контекста
- **Особенности:** OpenAI-совместимый API, высокая скорость генерации

**Модель для Query Transformation:**
- **Модель:** `accounts/fireworks/models/gpt-oss-120b`
- **Провайдер:** Fireworks AI
- **Назначение:** Улучшение поисковых запросов с учетом контекста диалога
- **Функция:** Переформулировка вопросов пользователя для более точного поиска

**Модель для RAGAS Evaluation:**
- **Модель:** `accounts/fireworks/models/gpt-oss-120b`
- **Провайдер:** Fireworks AI
- **Назначение:** Оценка качества ответов (метрики Faithfulness, Response Relevancy, Answer Correctness)

### Embeddings (Векторные представления)

**Основная модель embeddings:**
- **Модель:** `accounts/fireworks/models/qwen3-embedding-8b`
- **Провайдер:** Fireworks AI (через OpenAI-совместимый API)
- **Параметры:** 8B параметров
- **Размерность векторов:** Зависит от модели (обычно 1024-4096)
- **Назначение:** Семантический поиск документов
- **Особенности:** 
  - Поддержка множества языков, включая русский
  - Быстрая индексация через API
  - Не требует локальных ресурсов

**Альтернативная модель (HuggingFace):**
- **Модель:** `intfloat/multilingual-e5-base`
- **Провайдер:** HuggingFace (локальная)
- **Параметры:** 278M параметров
- **Размер:** ~1.1 GB
- **Устройство:** CPU (настраивается через `HUGGINGFACE_DEVICE`)
- **Назначение:** Локальная альтернатива для приватности и offline работы
- **Особенности:**
  - Retrieval Score: 67.14/100 (топ-32 из 180 моделей)
  - Поддержка 100+ языков
  - Работает на CPU без GPU
  - Оптимальный баланс качество/ресурсы

**Модель для RAGAS Evaluation:**
- **Модель:** `accounts/fireworks/models/qwen3-embedding-8b`
- **Провайдер:** Fireworks AI
- **Назначение:** Вычисление метрик Answer Similarity, Context Precision, Context Recall

### Reranking (Переранжирование)

**Cross-Encoder модель:**
- **Модель:** `cross-encoder/mmarco-mMiniLMv2-L12-H384-v1`
- **Провайдер:** HuggingFace
- **Параметры:** 117.6M параметров
- **Размер:** ~470 MB
- **Архитектура:** MiniLM v2, 12 слоев, 384 скрытых размерности
- **Назначение:** Точное переранжирование результатов hybrid retrieval
- **Особенности:**
  - Multilingual поддержка (включая русский)
  - Обучена на MS MARCO датасете
  - Работает на CPU
  - Оценивает пары (запрос, документ) для точного ранжирования

**Статус:** ❌ Не использовалась в экспериментах из-за ошибки загрузки модели (таймауты при скачивании из HuggingFace)

### Лексический поиск (BM25)

**Алгоритм:** BM25 (Best Matching 25)
- **Библиотека:** `rank-bm25`
- **Назначение:** Лексический поиск по точным совпадениям терминов
- **Особенности:**
  - Не требует обучения
  - Быстрый поиск по инвертированному индексу
  - Эффективен для точных терминов, номеров, названий
  - Используется в hybrid режиме для дополнения семантического поиска

### Сводная таблица моделей

| Компонент | Модель | Провайдер | Параметры | Размер | Статус |
|-----------|--------|-----------|-----------|--------|--------|
| **LLM (генерация)** | `gpt-oss-120b` | Fireworks | 120B | API | ✅ Используется |
| **LLM (query transform)** | `gpt-oss-120b` | Fireworks | 120B | API | ✅ Используется |
| **LLM (RAGAS)** | `gpt-oss-120b` | Fireworks | 120B | API | ✅ Используется |
| **Embeddings (основная)** | `qwen3-embedding-8b` | Fireworks | 8B | API | ✅ Используется |
| **Embeddings (альтернатива)** | `multilingual-e5-base` | HuggingFace | 278M | 1.1 GB | ⚠️ Fallback |
| **Embeddings (RAGAS)** | `qwen3-embedding-8b` | Fireworks | 8B | API | ✅ Используется |
| **Reranker** | `mmarco-mMiniLMv2-L12-H384-v1` | HuggingFace | 117.6M | 470 MB | ❌ Не загружена |
| **BM25** | Алгоритм | rank-bm25 | - | - | ✅ Используется |

### Выбор моделей

**Обоснование выбора:**

1. **Fireworks AI как основной провайдер:**
   - Высокая скорость генерации
   - OpenAI-совместимый API (легкая интеграция)
   - Доступные модели для русского языка
   - Удобное управление через единый API ключ

2. **qwen3-embedding-8b для embeddings:**
   - Хорошее качество для многоязычных задач
   - Быстрая работа через API
   - Не требует локальных ресурсов
   - Поддержка русского языка

3. **multilingual-e5-base как альтернатива:**
   - Лучшее качество среди локальных моделей (67.14/100)
   - Оптимальный баланс размер/качество
   - Работает на CPU
   - Полная приватность данных

4. **mmarco-mMiniLMv2 для reranking:**
   - Специализированная модель для reranking
   - Multilingual поддержка
   - Компактный размер (470 MB)
   - Обучена на MS MARCO (стандартный датасет для IR)

### Технические детали

**Инфраструктура:**
- **Python:** 3.13
- **Фреймворк RAG:** LangChain
- **Векторное хранилище:** InMemoryVectorStore (LangChain)
- **Оценка качества:** RAGAS
- **Мониторинг:** LangSmith
- **Управление зависимостями:** uv

## Конфигурации экспериментов

### 1. Semantic Retrieval

**Описание:** Векторный поиск по смыслу с использованием embeddings.

**Параметры:**
- `RETRIEVAL_MODE=semantic`
- `SEMANTIC_RETRIEVER_K=10`
- `EMBEDDING_PROVIDER=openai`
- `EMBEDDING_MODEL=accounts/fireworks/models/qwen3-embedding-8b`

**Принцип работы:**
- Запрос пользователя преобразуется в векторное представление через embeddings
- Выполняется поиск наиболее похожих документов по косинусному расстоянию
- Возвращаются топ-10 наиболее релевантных документов

### 2. Hybrid Retrieval

**Описание:** Комбинация семантического поиска и лексического поиска BM25.

**Параметры:**
- `RETRIEVAL_MODE=hybrid`
- `SEMANTIC_RETRIEVER_K=10`
- `BM25_RETRIEVER_K=10`
- `ENSEMBLE_SEMANTIC_WEIGHT=0.5`
- `ENSEMBLE_BM25_WEIGHT=0.5`
- `EMBEDDING_PROVIDER=openai`
- `EMBEDDING_MODEL=accounts/fireworks/models/qwen3-embedding-8b`

**Принцип работы:**
- Параллельно выполняются семантический поиск (10 документов) и BM25 поиск (10 документов)
- Результаты объединяются с весами 0.5 для каждого метода
- Возвращаются топ-10 документов после объединения

### 3. Hybrid + Reranker

**Описание:** Hybrid retrieval с дополнительным ранжированием через Cross-Encoder.

**Параметры:**
- `RETRIEVAL_MODE=hybrid_reranker`
- `SEMANTIC_RETRIEVER_K=10`
- `BM25_RETRIEVER_K=10`
- `ENSEMBLE_SEMANTIC_WEIGHT=0.5`
- `ENSEMBLE_BM25_WEIGHT=0.5`
- `CROSS_ENCODER_MODEL=cross-encoder/mmarco-mMiniLMv2-L12-H384-v1`
- `RERANKER_TOP_K=3`
- `EMBEDDING_PROVIDER=openai`
- `EMBEDDING_MODEL=accounts/fireworks/models/qwen3-embedding-8b`

**Принцип работы:**
- Выполняется hybrid retrieval (семантический + BM25)
- Результаты дополнительно ранжируются через Cross-Encoder модель
- Возвращаются топ-3 наиболее релевантных документа

**Статус:** ❌ Не запущен из-за ошибки загрузки модели HuggingFace (проблема с сетью/таймаутами при скачивании модели `cross-encoder/mmarco-mMiniLMv2-L12-H384-v1`)

## Результаты экспериментов

### Метрики RAGAS

Были использованы следующие метрики для оценки качества:

- **Answer Relevancy** - релевантность ответа запросу пользователя
- **Answer Similarity** - схожесть ответа с эталонным ответом
- **Context Precision** - точность контекста (доля релевантных документов среди извлеченных)
- **Context Recall** - полнота контекста (доля релевантных документов, которые были найдены)

### Таблица результатов

| Эксперимент | Answer Relevancy | Answer Similarity | Context Precision | Context Recall |
|-------------|------------------|-------------------|-------------------|----------------|
| **Semantic** | - | 0.13 | 0.00 | 0.33 |
| **Hybrid** | 0.00 | 0.02 | 0.00 | 0.00 |
| **Hybrid + Reranker** | - | - | - | - |

*Примечание: Значения метрик в диапазоне [0, 1], где 1 - наилучший результат*

## Сравнительный анализ

### Semantic Retrieval

**Сильные стороны:**
- ✅ Показал лучший результат по **Context Recall (0.33)** - система находит больше релевантных документов
- ✅ **Answer Similarity (0.13)** - ответы имеют некоторую схожесть с эталонными

**Слабые стороны:**
- ❌ **Context Precision (0.00)** - все извлеченные документы нерелевантны (ложные срабатывания)
- ❌ Отсутствует метрика Answer Relevancy

**Анализ:**
Semantic retrieval находит больше релевантных документов (высокий recall), но при этом извлекает много нерелевантных документов (низкий precision). Это типичная проблема семантического поиска - он может находить документы, которые семантически близки, но не отвечают на конкретный вопрос.

### Hybrid Retrieval

**Сильные стороны:**
- ✅ Комбинация двух методов поиска должна давать более сбалансированные результаты

**Слабые стороны:**
- ❌ Все метрики близки к нулю или равны нулю
- ❌ **Answer Relevancy (0.00)** - ответы не релевантны запросам
- ❌ **Answer Similarity (0.02)** - очень низкая схожесть с эталоном
- ❌ **Context Precision (0.00)** и **Context Recall (0.00)** - система не находит релевантные документы

**Анализ:**
Результаты Hybrid retrieval оказались хуже, чем у Semantic. Возможные причины:
1. Неправильная настройка весов ensemble (0.5/0.5 может быть не оптимальным)
2. BM25 может конфликтовать с семантическим поиском для данной задачи
3. Проблемы с объединением результатов двух методов
4. Недостаточное количество тестовых примеров для корректной оценки

### Hybrid + Reranker

**Статус:** ❌ Эксперимент не выполнен

**Причина:** Ошибка при загрузке модели Cross-Encoder из HuggingFace:
- Таймауты при скачивании модели `cross-encoder/mmarco-mMiniLMv2-L12-H384-v1`
- Проблемы с сетью/прокси
- Модель не загружается локально

**Ожидаемые результаты:**
Теоретически, reranking должен был улучшить Context Precision, отфильтровав нерелевантные документы из результатов hybrid retrieval.

## Выводы

### Лучшая конфигурация

**Semantic Retrieval** показал лучшие результаты среди протестированных конфигураций:

1. **Context Recall (0.33)** - система находит треть релевантных документов, что является лучшим показателем
2. **Answer Similarity (0.13)** - ответы имеют некоторую схожесть с эталонными, что лучше, чем у Hybrid (0.02)

### Рекомендации

1. **Для production:** Использовать **Semantic Retrieval** как основную конфигурацию
   - Настроить `SEMANTIC_RETRIEVER_K` для оптимизации баланса precision/recall
   - Рассмотреть увеличение количества извлекаемых документов для улучшения recall

2. **Для улучшения Hybrid:**
   - Экспериментировать с весами ensemble (например, 0.7 для semantic, 0.3 для BM25)
   - Настроить параметры BM25 (k1, b) под специфику документов
   - Увеличить размер датасета для более корректной оценки

3. **Для Hybrid + Reranker:**
   - Решить проблему с загрузкой модели Cross-Encoder
   - Рассмотреть альтернативные модели reranking
   - Использовать предзагруженные модели или кэширование

### Ограничения эксперимента

1. **Размер датасета:** Небольшой размер тестового датасета (10 вопросов) может не давать статистически значимых результатов
2. **Отсутствие метрик:** Не все метрики были вычислены для всех экспериментов
3. **Технические проблемы:** Hybrid + Reranker не был протестирован из-за проблем с инфраструктурой
4. **Модель embeddings:** Использовалась модель Fireworks, которая может быть слабее специализированных моделей для русского языка

### Дальнейшие шаги

1. Увеличить размер датасета для более надежной оценки
2. Протестировать другие модели embeddings (например, `intfloat/multilingual-e5-large`)
3. Решить проблему с загрузкой Cross-Encoder модели
4. Провести A/B тестирование в production для валидации результатов
5. Настроить гиперпараметры для оптимизации метрик

## Заключение

Среди протестированных конфигураций **Semantic Retrieval** показал наилучшие результаты, особенно по метрике Context Recall. Hybrid retrieval показал неожиданно низкие результаты, что требует дополнительного исследования и настройки. Hybrid + Reranker не был протестирован из-за технических проблем, но теоретически должен улучшить качество за счет более точного ранжирования результатов.

---

**Дата проведения экспериментов:** Ноябрь 2024  
**Инструменты оценки:** RAGAS  
**Датасет:** 10 вопросов (8 о кредитах, 2 о вкладах)

