# Ручное создание датасета для оценки RAG

## Структура датасета

Датасет должен быть сохранен в файле `datasets/06-rag-qa-dataset.json` и содержать минимум 6-8 Q&A пар.

### Формат каждой записи:

```json
{
  "question": "Вопрос по документу",
  "ground_truth": "Правильный ответ",
  "contexts": ["Релевантный текст из документа"],
  "metadata": {
    "source": "filename.pdf",
    "page": 3,
    "type": "manual"
  }
}
```

### Поля:

- **question** (строка) - Вопрос, который пользователь может задать боту
- **ground_truth** (строка) - Правильный ответ, который должен дать бот
- **contexts** (массив строк) - Релевантные фрагменты текста из документов, которые содержат информацию для ответа
- **metadata** (объект):
  - **source** (строка) - Имя файла источника (например, "ouk_potrebitelskiy_kredit_lph.pdf" или "sberbank_help_documents.json")
  - **page** (число или null) - Номер страницы PDF (если применимо)
  - **type** (строка) - "manual" для ручно созданных записей
  - **category** (строка, опционально) - Категория вопроса

## Процесс создания датасета вручную

### Шаг 1: Изучите документы

Изучите документы в директории `data/`:
- PDF файлы: `ouk_potrebitelskiy_kredit_lph.pdf`, `usl_r_vkladov.pdf`
- JSON файл: `sberbank_help_documents.json`

### Шаг 2: Создайте вопросы

Для каждого документа создайте реалистичные вопросы:
- Вопросы должны быть такими, которые могут задать реальные пользователи
- Вопросы должны быть конкретными (можно дать точный ответ)
- Вопросы должны быть на русском языке
- Разнообразьте типы вопросов (общие, конкретные, процедурные)

**Рекомендации:**
- По 2-3 вопроса из каждого PDF файла
- 2-4 вопроса из JSON файла с готовыми Q&A парами
- Всего минимум 6-8 вопросов

### Шаг 3: Напишите правильные ответы (ground truth)

Для каждого вопроса:
- Напишите точный и полный ответ на основе документа
- Ответ должен быть основан только на информации из документов
- Ответ должен быть информативным и полезным

### Шаг 4: Укажите релевантные контексты

Для каждого вопроса укажите фрагменты текста из документов:
- Контексты должны содержать информацию, необходимую для ответа
- Можно указать один или несколько фрагментов
- Фрагменты должны быть достаточно полными (200-500 символов)

### Шаг 5: Заполните метаданные

Для каждой записи заполните:
- **source**: имя файла источника
- **page**: номер страницы (если вопрос из PDF)
- **type**: "manual" для ручно созданных записей
- **category**: категория вопроса (опционально)

### Шаг 6: Сохраните в JSON

Создайте файл `datasets/06-rag-qa-dataset.json` и сохраните массив записей:

```json
[
  {
    "question": "...",
    "ground_truth": "...",
    "contexts": ["..."],
    "metadata": {
      "source": "...",
      "page": 3,
      "type": "manual"
    }
  },
  ...
]
```

### Шаг 7: Проверьте формат

Убедитесь, что:
- JSON валидный (можно проверить через онлайн-валидатор)
- Все обязательные поля заполнены
- Минимум 6-8 записей
- `contexts` - это массив строк (даже если одна строка)

## Пример готового датасета

См. файл `datasets/06-rag-qa-dataset-manual-template.json` для примера структуры.

## Использование готового датасета

После создания датасета вы можете:

1. **Загрузить датасет в LangSmith:**
   ```bash
   make dataset-upload
   ```

2. **Запустить оценку через Telegram:**
   - Отправьте команду `/evaluate` боту
   - Бот запустит evaluation на основе датасета

3. **Просмотреть результаты в LangSmith:**
   - Откройте https://smith.langchain.com
   - Перейдите в проект `06-rag-assistant`
   - Просмотрите трейсы и метрики оценки

## Альтернатива: Автоматический синтез

Если не хотите создавать датасет вручную, можно использовать автоматический синтез:

```bash
make dataset
```

Это создаст датасет автоматически на основе документов, но качество может быть ниже, чем при ручном создании.

